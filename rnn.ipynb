{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCg-aIZN5MdA"
      },
      "source": [
        "library imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "13h8yO_05MdH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n",
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.\n",
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "produce text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scraper import S\n",
        "s = S()\n",
        "s.make_query()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5l5qzyD5MdL"
      },
      "source": [
        "explore text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIh3DRmW5MdM",
        "outputId": "ae079f52-0c45-45c4-c55f-edaee0a4fb33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 95812 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open('raw.txt', 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlO-DgiA5MdN"
      },
      "source": [
        "first characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VDq2sr45MdO",
        "outputId": "c04befcb-f7de-44c0-87b7-e8b7751cad5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nd outdoor, tease, indulge, and play harmless pranks on each other. The acquisition of love and attention itself may be part of the game.[6]\n",
            "Ludic lovers want to have as much fun as possible. When they are not seeking a stable relationship, they rare\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTd2EGFs5MdP"
      },
      "source": [
        "get a list of unique characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k19zTO8l5MdQ",
        "outputId": "726c533e-f135-4b60-8aa6-a80f1ff38064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "unique = sorted(set(text))\n",
        "print(f'{len(unique)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r71GyvYv5MdR"
      },
      "source": [
        "vectorize text (convert strings to a numerical representation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhOhXAX75MdS",
        "outputId": "491eb3a8-3b57-4dae-8c40-335e52bdb4e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# split text into tokens\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUkRSt6O5MdS"
      },
      "source": [
        "add stringlookup layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1SJt4hQ5MdT",
        "outputId": "d886862d-441f-4ae6-b445-bb75e3e1cd64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[58, 59, 60, 61, 62, 63, 64], [81, 82, 83]]>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(unique), mask_token=None)\n",
        "    \n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9lLMLbk5MdU"
      },
      "source": [
        "initialize layer for reverse operation (vector to text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpJpB4gd5MdU",
        "outputId": "d31cef68-8f1b-4ff5-ab3f-35085575e2e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
        "    \n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzdt5eYv5MdV"
      },
      "source": [
        "join bit chars into strings with the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0I8aO_75MdV",
        "outputId": "96939010-89df-4d5d-ea2e-4ee8a0bb4cb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgj17ha-5MdV"
      },
      "source": [
        "define the inverse operation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wSqblDe5MdW"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGxm5oGP5MdW"
      },
      "source": [
        "vectorize all text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke1GzWc05MdX",
        "outputId": "630ff757-5cc7-4269-f56c-d69d256388cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1171272,), dtype=int64, numpy=array([1, 2, 1, ..., 1, 2, 1])>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWpUrcze5MdX"
      },
      "source": [
        "dump text into a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7Y6qlBU5MdY",
        "outputId": "29972c1e-1c56-4882-f9d9-87d0945b1782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            "\r\n",
            "\n",
            "\n",
            "C\n",
            "R\n",
            "I\n",
            "M\n",
            "E\n"
          ]
        }
      ],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B52O78m35MdY"
      },
      "source": [
        "convert individual characters to sequences of size 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QABMwbM5MdY",
        "outputId": "59f3ba35-30bc-45f8-a6c4-b38231af7393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'\\n' b'\\r' b'\\n' b'\\r' b'\\n' b'C' b'R' b'I' b'M' b'E' b' ' b'A' b'N'\n",
            " b'D' b' ' b'P' b'U' b'N' b'I' b'S' b'H' b'M' b'E' b'N' b'T' b'\\r' b'\\n'\n",
            " b'\\r' b'\\n' b'\\r' b'\\n' b'\\r' b'\\n' b'\\r' b'\\n' b'P' b'A' b'R' b'T' b' '\n",
            " b'I' b'\\r' b'\\n' b'\\r' b'\\n' b'\\r' b'\\n' b'\\r' b'\\n' b'C' b'H' b'A' b'P'\n",
            " b'T' b'E' b'R' b' ' b'I' b'\\r' b'\\n' b'\\r' b'\\n' b'O' b'n' b' ' b'a' b'n'\n",
            " b' ' b'e' b'x' b'c' b'e' b'p' b't' b'i' b'o' b'n' b'a' b'l' b'l' b'y'\n",
            " b' ' b'h' b'o' b't' b' ' b'e' b'v' b'e' b'n' b'i' b'n' b'g' b' ' b'e'\n",
            " b'a' b'r' b'l' b'y' b' ' b'i'], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdisfvZ95Mda"
      },
      "source": [
        "join sequences back into strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r_SIiXi5Mda",
        "outputId": "46f062e6-6e97-40c0-a478-9c2931887a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'\\n\\r\\n\\r\\nCRIME AND PUNISHMENT\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nPART I\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\nOn an exceptionally hot evening early i'\n",
            "b'n July a young man came out of\\r\\nthe garret in which he lodged in S. Place and walked slowly, as thoug'\n",
            "b'h\\r\\nin hesitation, towards K. bridge.\\r\\n\\r\\nHe had successfully avoided meeting his landlady on the stair'\n",
            "b'case. His\\r\\ngarret was under the roof of a high, five-storied house and was more\\r\\nlike a cupboard than'\n",
            "b' a room. The landlady who provided him with garret,\\r\\ndinners, and attendance, lived on the floor belo'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3y1FUrn5Mdb"
      },
      "source": [
        "define 'split_input_target', a function which, at each time step, takes input as the current character and label as the next character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLQNu5lS5Mdb",
        "outputId": "120014b8-e885-4a36-82e9-0f7bd1e53a01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6aEYKc15Mdb"
      },
      "source": [
        "map function across our sequences, save to dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBFkpUTL5Mdc",
        "outputId": "1ef751f2-1aa0-443f-94aa-071ce8f5bde1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'\\n\\r\\n\\r\\nCRIME AND PUNISHMENT\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nPART I\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\nOn an exceptionally hot evening early '\n",
            "Target: b'\\r\\n\\r\\nCRIME AND PUNISHMENT\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nPART I\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\nOn an exceptionally hot evening early i'\n"
          ]
        }
      ],
      "source": [
        "dataset = sequences.map(split_input_target)\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKAKKFsY5Mdc"
      },
      "source": [
        "shuffle and pack data into batches for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2837eyR5Mdc",
        "outputId": "04712c31-72c5-4ba4-ac8c-f839cb5a53a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMqySNX65Mdd"
      },
      "source": [
        "build 3-layer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "szEacEfy5Mdd"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ids_from_chars' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e8edb0f702d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Length of the vocabulary in StringLookup Layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_from_chars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# The embedding dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ids_from_chars' is not defined"
          ]
        }
      ],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "# Batch size\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.rnn_units = rnn_units\n",
        "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')\n",
        "        self.dense = layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "            x, states = self.gru(x, initial_state=states, training=training)\n",
        "            x = self.dense(x, training=training)\n",
        "\n",
        "            if return_state:\n",
        "                return x, states\n",
        "            else:\n",
        "                return x\n",
        "\n",
        "model = Model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RG8gM-w75Mde"
      },
      "outputs": [],
      "source": [
        "# class MyModel(tf.keras.Model):\n",
        "#   def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "#     super().__init__(self)\n",
        "#     self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "#     self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "#                                    return_sequences=True,\n",
        "#                                    return_state=True)\n",
        "#     self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "#   def call(self, inputs, states=None, return_state=False, training=False):\n",
        "#     x = inputs\n",
        "#     x = self.embedding(x, training=training)\n",
        "#     if states is None:\n",
        "#       states = self.gru.get_initial_state(x)\n",
        "#     x, states = self.gru(x, initial_state=states, training=training)\n",
        "#     x = self.dense(x, training=training)\n",
        "\n",
        "#     if return_state:\n",
        "#       return x, states\n",
        "#     else:\n",
        "#       return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErMUYQLQ5Mdf"
      },
      "outputs": [],
      "source": [
        "# model = MyModel(\n",
        "#     vocab_size=vocab_size,\n",
        "#     embedding_dim=embedding_dim,\n",
        "#     rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV8a8gQ55Mdf",
        "outputId": "9ebe3b72-2fa8-4cb0-84d9-551081f09a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 100) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ofDtto75Mdg",
        "outputId": "633c861f-8dc0-4c7f-de8b-f88ec7d0b188"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  25600     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  102500    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,066,404\n",
            "Trainable params: 4,066,404\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpnKmE225Mdg"
      },
      "source": [
        "make predictions by sampling from output distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXKEROp-5Mdg",
        "outputId": "f7eaa16c-afc3-4d9c-e5b8-260522a8eae9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 6, 18, 84, 78, 95, 23, 76, 48,  9,  9, 10, 98, 85, 40, 91, 72, 36,\n",
              "        2, 29, 49, 99, 91, 49, 96, 45, 29, 77, 60, 92, 80, 43,  0, 17, 22,\n",
              "       62, 31,  3, 61, 21, 44, 42, 96,  4,  1, 20, 13, 48, 98, 12, 85, 75,\n",
              "       25, 69, 90, 82, 87, 16, 47,  1, 31,  3,  2, 21, 31, 44, 98, 88, 73,\n",
              "        4, 40, 44, 99, 94, 57, 23,  1, 97, 75, 59, 96, 70, 81, 94, 54, 21,\n",
              "       26, 61, 89, 20, 61, 61,  6, 94, 36, 35, 32, 26, 86, 52, 15])"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
        "\n",
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0QjDOBD5Mdh"
      },
      "source": [
        "decode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYjyvPE25Mdh",
        "outputId": "10d80540-4837-4051-9b77-4e5062e14910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'\\r\\nanything more.\\xe2\\x80\\x99 I am telling you Dushkin\\xe2\\x80\\x99s story. \\xe2\\x80\\x98I gave him a note\\xe2\\x80\\x99--a\\r\\nrouble that is--\\xe2\\x80\\x98for I t'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'$2\\xc3\\xa0u\\xc3\\xbc7sT(()\\xe2\\x80\\x9c\\xc3\\xa4L\\xc3\\xaeoH\\rAU\\xe2\\x80\\x9d\\xc3\\xaeU\\xe2\\x80\\x98QAtc\\xc3\\xafwO[UNK]16eC d5PN\\xe2\\x80\\x98!\\n4-T\\xe2\\x80\\x9c,\\xc3\\xa4r9l\\xc3\\xaay\\xc3\\xa70S\\nC \\r5CP\\xe2\\x80\\x9c\\xc3\\xa8p!LP\\xe2\\x80\\x9d\\xc3\\xb6_7\\n\\xe2\\x80\\x99rb\\xe2\\x80\\x98mx\\xc3\\xb6Z5:d\\xc3\\xa94dd$\\xc3\\xb6HGD:\\xc3\\xa6X/'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EahkMyMq5Mdh"
      },
      "source": [
        "add optimizer, loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6KGLi_D5Mdi"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-luf7e45Mdi",
        "outputId": "7f2a5ee2-8812-45ac-c811-620cf0b7cacc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 100)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.6051164, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wRXiQ165Mdj",
        "outputId": "a3781041-fbdb-45ec-d16c-845c5cac0442"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "99.99461"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#should be roughly equal to vocab size, 99\n",
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qjqoX2M5Mdj"
      },
      "source": [
        "compile model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ToxFTzd5Mdj"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSqGLtLP5Mdk"
      },
      "source": [
        "add checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48NKI63T5Mdk"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEXf3BdR5Mdk",
        "outputId": "8d485716-63bc-4de6-ca52-33a4c0b8a69b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "181/181 [==============================] - 12s 53ms/step - loss: 2.6634\n",
            "Epoch 2/100\n",
            "181/181 [==============================] - 11s 54ms/step - loss: 1.9266\n",
            "Epoch 3/100\n",
            "181/181 [==============================] - 11s 54ms/step - loss: 1.5932\n",
            "Epoch 4/100\n",
            "181/181 [==============================] - 11s 55ms/step - loss: 1.4071\n",
            "Epoch 5/100\n",
            "181/181 [==============================] - 12s 55ms/step - loss: 1.3008\n",
            "Epoch 6/100\n",
            "181/181 [==============================] - 11s 56ms/step - loss: 1.2297\n",
            "Epoch 7/100\n",
            "181/181 [==============================] - 11s 57ms/step - loss: 1.1762\n",
            "Epoch 8/100\n",
            "181/181 [==============================] - 11s 57ms/step - loss: 1.1304\n",
            "Epoch 9/100\n",
            "181/181 [==============================] - 11s 57ms/step - loss: 1.0884\n",
            "Epoch 10/100\n",
            "181/181 [==============================] - 11s 57ms/step - loss: 1.0489\n",
            "Epoch 11/100\n",
            "181/181 [==============================] - 11s 57ms/step - loss: 1.0096\n",
            "Epoch 12/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.9704\n",
            "Epoch 13/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.9296\n",
            "Epoch 14/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.8884\n",
            "Epoch 15/100\n",
            "181/181 [==============================] - 11s 57ms/step - loss: 0.8445\n",
            "Epoch 16/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.8004\n",
            "Epoch 17/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.7572\n",
            "Epoch 18/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.7147\n",
            "Epoch 19/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.6755\n",
            "Epoch 20/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.6381\n",
            "Epoch 21/100\n",
            "181/181 [==============================] - 11s 57ms/step - loss: 0.6049\n",
            "Epoch 22/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.5761\n",
            "Epoch 23/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.5491\n",
            "Epoch 24/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.5287\n",
            "Epoch 25/100\n",
            "181/181 [==============================] - 11s 56ms/step - loss: 0.5108\n",
            "Epoch 26/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4929\n",
            "Epoch 27/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4814\n",
            "Epoch 28/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4708\n",
            "Epoch 29/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4614\n",
            "Epoch 30/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4530\n",
            "Epoch 31/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4479\n",
            "Epoch 32/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4455\n",
            "Epoch 33/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4406\n",
            "Epoch 34/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4374\n",
            "Epoch 35/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4330\n",
            "Epoch 36/100\n",
            "181/181 [==============================] - 11s 57ms/step - loss: 0.4305\n",
            "Epoch 37/100\n",
            "181/181 [==============================] - 11s 56ms/step - loss: 0.4295\n",
            "Epoch 38/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4310\n",
            "Epoch 39/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4336\n",
            "Epoch 40/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4291\n",
            "Epoch 41/100\n",
            "181/181 [==============================] - 11s 57ms/step - loss: 0.4299\n",
            "Epoch 42/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4344\n",
            "Epoch 43/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4338\n",
            "Epoch 44/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4388\n",
            "Epoch 45/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4430\n",
            "Epoch 46/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4419\n",
            "Epoch 47/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4368\n",
            "Epoch 48/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4408\n",
            "Epoch 49/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4469\n",
            "Epoch 50/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4530\n",
            "Epoch 51/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4579\n",
            "Epoch 52/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4571\n",
            "Epoch 53/100\n",
            "181/181 [==============================] - 11s 57ms/step - loss: 0.4602\n",
            "Epoch 54/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4614\n",
            "Epoch 55/100\n",
            "181/181 [==============================] - 12s 59ms/step - loss: 0.4686\n",
            "Epoch 56/100\n",
            "181/181 [==============================] - 12s 59ms/step - loss: 0.4742\n",
            "Epoch 57/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4795\n",
            "Epoch 58/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.4889\n",
            "Epoch 59/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.4960\n",
            "Epoch 60/100\n",
            "181/181 [==============================] - 12s 60ms/step - loss: 0.5070\n",
            "Epoch 61/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.5092\n",
            "Epoch 62/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.5101\n",
            "Epoch 63/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.5140\n",
            "Epoch 64/100\n",
            "181/181 [==============================] - 12s 59ms/step - loss: 0.5171\n",
            "Epoch 65/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.5245\n",
            "Epoch 66/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.5397\n",
            "Epoch 67/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.5551\n",
            "Epoch 68/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.5635\n",
            "Epoch 69/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.5663\n",
            "Epoch 70/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.5746\n",
            "Epoch 71/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.5748\n",
            "Epoch 72/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.5838\n",
            "Epoch 73/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.5979\n",
            "Epoch 74/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.6059\n",
            "Epoch 75/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.6220\n",
            "Epoch 76/100\n",
            "181/181 [==============================] - 11s 57ms/step - loss: 0.6444\n",
            "Epoch 77/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.6731\n",
            "Epoch 78/100\n",
            "181/181 [==============================] - 12s 59ms/step - loss: 0.6717\n",
            "Epoch 79/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.6749\n",
            "Epoch 80/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.6828\n",
            "Epoch 81/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.6891\n",
            "Epoch 82/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.7212\n",
            "Epoch 83/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.7467\n",
            "Epoch 84/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.7489\n",
            "Epoch 85/100\n",
            "181/181 [==============================] - 12s 59ms/step - loss: 0.7520\n",
            "Epoch 86/100\n",
            "181/181 [==============================] - 12s 59ms/step - loss: 0.7789\n",
            "Epoch 87/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.7848\n",
            "Epoch 88/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.8047\n",
            "Epoch 89/100\n",
            "181/181 [==============================] - 11s 56ms/step - loss: 0.8175\n",
            "Epoch 90/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.8295\n",
            "Epoch 91/100\n",
            "181/181 [==============================] - 12s 59ms/step - loss: 0.8609\n",
            "Epoch 92/100\n",
            "181/181 [==============================] - 12s 60ms/step - loss: 0.8891\n",
            "Epoch 93/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.9030\n",
            "Epoch 94/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 0.9062\n",
            "Epoch 95/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.9167\n",
            "Epoch 96/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.9238\n",
            "Epoch 97/100\n",
            "181/181 [==============================] - 12s 58ms/step - loss: 0.9356\n",
            "Epoch 98/100\n",
            "181/181 [==============================] - 12s 60ms/step - loss: 0.9587\n",
            "Epoch 99/100\n",
            "181/181 [==============================] - 12s 59ms/step - loss: 0.9893\n",
            "Epoch 100/100\n",
            "181/181 [==============================] - 12s 57ms/step - loss: 1.0196\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 100\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lvSveKd5Mdl"
      },
      "source": [
        "generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zluTrc5A5Mdl"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtQfSX-m5Mdm",
        "outputId": "993bca1d-8f56-4d2d-e1ca-12db6f07c5fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The world seemed like such a peaceful place until the magic tree was discovered in London.\r\n",
            "\r\n",
            "He had not chasted in Loze him extreme coldirm the collection of Polenka and proved it against his\r\n",
            "face. He heard their cases to the Project\r\n",
            "\r\n",
            " God granted, I... and as for your plare you will help out to uttered both hands in\r\n",
            "every weeping.). It does it matter of compliance with my sons!”\r\n",
            "\r\n",
            "“Yes, you are sitting on her slip. This murder,\r\n",
            "she cried irritating. “It was. I’ve come to be different and refuence,\r\n",
            "imaginary men money with me or indinged), but that grewoment, who had\r\n",
            "to defe it in your position,” he observed gloomy roubles pension, as though not\r\n",
            "understood genially. “I have bound him,\r\n",
            "feeling very sensibly, to very requirent facts, the same fuen into a vigorous tymandar; there were\r\n",
            "so unexpectedly, terrible, if you expect upon your sense if you are minute, you\r\n",
            "were twise...”\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "CHAPTER IV\r\n",
            "\r\n",
            "Sonia stopped on her derangemptailiand was standing in despair.\r\n",
            "\r\n",
            "“Never,” on him, the daughters of gentlemen in Maranu’t he one. ‘The\r\n",
            "fire you, Rodya.” He was hungle \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.455770492553711\n"
          ]
        }
      ],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant([''])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "dostoevsky.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "742421778fb6edbb2ea867496507c78fad36726f11b6e4581b3dfcf5377afbd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
